{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bd6bce6",
   "metadata": {},
   "source": [
    "# Set up Mediapipe and camera functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3730e0f",
   "metadata": {},
   "source": [
    "The goal of this project is to make a simple PacMan game that can be played by using hand gestures, employing computer vision and machine learning for recognition and prediction. To that end, this notebook will serve to (1) collect the necessary data, and (2) make and test the machine learning model.\n",
    "\n",
    "First, it is necessary to set up the camera that can accept a live video feed, and implement hand tracking to enable gesture recognition. I will use OpenCV to open and process the camera feed, and Google's MediaPipe to enable hand tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deeaff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all necessary libraries\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapipe as mp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv2D, MaxPooling2D, Input, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfbf64e",
   "metadata": {},
   "source": [
    "First of all, we need to set up MediaPipe. Since I wanted to experiment with different gesture-tracking setups, I decided to use the holistic module which can detect face, pose, and hands. I assumed the pose especially would be valuable, as my original plan was to use an LSTM model to track longer sequences where the hands' positions relative to the body would be important. In the end, I only needed the hands but decided to keep the holistic model in case I want to employ more poses or gestures in the future.\n",
    "\n",
    "We also need to set up the drawing utilities that will enable us to overlay the keypoints that MediaPipie detects over the live video feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04817250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# holistic model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "# drawing utilities\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39961819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp_detection(img, model):\n",
    "    '''\n",
    "    Process an image with the passed MediaPipe model and track keypoints specified by the model.\n",
    "    \n",
    "        Parameters:\n",
    "            img (numpy.ndarray): an array representing an image\n",
    "            model (MediaPipe Model): a custom MediaPipe class representing a model\n",
    "            \n",
    "        Returns:\n",
    "            img (numpy.ndarray): an array representing the video frame equal to the input img\n",
    "            results (NamedTuple): a NamedTuple containing the face, pose, and left and right hand landmarks detected \n",
    "                by MediaPipe's Holistic model\n",
    "    '''\n",
    "    \n",
    "    # cv2 captures video in BGR - have to convert to RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # stop writing to the image\n",
    "    img.flags.writeable = False\n",
    "    \n",
    "    # make a prediction\n",
    "    results = model.process(img)\n",
    "    \n",
    "    # start writing to the image again\n",
    "    img.flags.writeable = True\n",
    "    \n",
    "    # convert the image back to BGR\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    return img, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43f7edb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(img, results):\n",
    "    '''\n",
    "    Draw keypoints detected by MediaPipe on top of the passed image.\n",
    "    \n",
    "        Parameters:\n",
    "            img (numpy.ndarray): an array representing an image\n",
    "            results (NamedTuple): a NamedTuple containing the face, pose, and left and right hand landmarks detected \n",
    "                by MediaPipe's Holistic model\n",
    "                \n",
    "        Returns:\n",
    "            void\n",
    "    '''\n",
    "\n",
    "    # draw left and right hand connections\n",
    "    mp_drawing.draw_landmarks(img, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(0, 128, 0), thickness=2, circle_radius=2),\n",
    "                             mp_drawing.DrawingSpec(color=(0, 128, 0), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    mp_drawing.draw_landmarks(img, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=2),\n",
    "                             mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=2)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef80ce0",
   "metadata": {},
   "source": [
    "Having prepared the MediaPipe helper functions, we can now access the webcam through OpenCV and make sure that everything is set up correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b690126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access webcam\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "# access mediapipe holistic model\n",
    "# set initial detection confidence and subsequent tracking confidence\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while capture.isOpened():\n",
    "\n",
    "        # read the video\n",
    "        ret, frame = capture.read()\n",
    "\n",
    "        # make detections with the holistic model\n",
    "        image, results = mp_detection(frame, holistic)\n",
    "        \n",
    "        # draw landmarks on the live feed\n",
    "        draw_landmarks(image, results)\n",
    "\n",
    "        # show the video in frame\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # escape the video feed\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78a282cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr of keypoints in pose: 33\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'landmark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNr of keypoints in pose: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark)))\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNr of keypoints in hand: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft_hand_landmarks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlandmark\u001b[49m)))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'landmark'"
     ]
    }
   ],
   "source": [
    "print('Nr of keypoints in pose: ' + str(len(results.pose_landmarks.landmark)))\n",
    "print('Nr of keypoints in face: ' + str(len(results.face_landmarks.landmark)))\n",
    "print('Nr of keypoints in hand: ' + str(len(results.left_hand_landmarks.landmark)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a511928e",
   "metadata": {},
   "source": [
    "# Collect outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e91a10",
   "metadata": {},
   "source": [
    "Now that everything is set up, we can start collecting data for the gesture recognition model. For that, we first need to set up a function that extracts only the keypoints that we are interested in from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19a795bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_hand_keypoints(results):\n",
    "    '''\n",
    "    Get the keypoints that represent a hand detected by MediaPipe. Only read one hand at a time, and prioritise right hand.\n",
    "    \n",
    "        Parameters:\n",
    "            results (NamedTuple): a NamedTuple containing the face, pose, and left and right hand landmarks detected \n",
    "                by MediaPipe's Holistic model\n",
    "        \n",
    "        Returns:\n",
    "            keypoints (numpy.ndarray): an array of shape (21 keypoints, 3 dimensions) representing the keypoints in \n",
    "                the hand detected by MediaPipe's Holistic model\n",
    "    '''\n",
    "    \n",
    "    # get one hand's keypoint if the other hand is not in frame at all\n",
    "    if results.left_hand_landmarks == None and results.right_hand_landmarks != None:\n",
    "        keypoints = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark])\n",
    "    elif results.right_hand_landmarks == None and results.left_hand_landmarks != None:\n",
    "        keypoints = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark])\n",
    "        \n",
    "    # get one hand's keypoints if the other hand is in frame only partially\n",
    "    elif (results.right_hand_landmarks != None and results.left_hand_landmarks != None) and (np.count_nonzero(results.right_hand_landmarks.landmark) >= np.count_nonzero(results.left_hand_landmarks.landmark)):\n",
    "        keypoints = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark])\n",
    "    elif (results.right_hand_landmarks != None and results.left_hand_landmarks != None) and (np.count_nonzero(results.left_hand_landmarks.landmark) > np.count_nonzero(results.right_hand_landmarks.landmark)):\n",
    "        keypoints = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark])\n",
    "        \n",
    "    # return an array full of zeros if no hands are in the frame at all\n",
    "    else:\n",
    "        keypoints = np.zeros(21*3)\n",
    "    \n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eec771",
   "metadata": {},
   "source": [
    "Now we can make a folder to store the keypoint arrays and start data collection for each action we want to detect. Since we need to detect gestures to play PacMan, we need four different gestures representing directions right, left, up and down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1017fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for exported arrays\n",
    "PATH = os.path.join('data-single-hand')\n",
    "\n",
    "# setting up collecting actions\n",
    "actions = np.array(['right', 'left', 'up', 'down'])\n",
    "\n",
    "# seq_nr videos to collect data from for each action\n",
    "seq_nr = 500\n",
    "\n",
    "# each video is seq_len frames long\n",
    "seq_len = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "7176640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up folders for each video\n",
    "for action in actions:\n",
    "    for seq in range(seq_nr*2):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(PATH, action, str(seq)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "cd3b4121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the actual training videos for individual actions\n",
    "\n",
    "# access webcam\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "# access mediapipe holistic model\n",
    "# set initial detection confidence and subsequent tracking confidence\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    # loop through the 4 actions\n",
    "    for action in actions:\n",
    "        \n",
    "        if action != actions[0]:\n",
    "            # 5 second break before recording each action besides the first one\n",
    "            cv2.waitKey(5000)\n",
    "        \n",
    "        # loop through the number of videos we want to collect for each action\n",
    "        # on first go, use range(0, seq_nr) and record all actions done by the left hand,\n",
    "        # and on second go, use range(seq_nr, seq_nr*2) and record all actions done by the right hand\n",
    "        for sequence in range(seq_nr, seq_nr*2):\n",
    "\n",
    "            # loop through video length\n",
    "            # in the end I did not use an LSTM model so I only needed individual frames, \n",
    "            # making seq_len = 1\n",
    "            for frame_nr in range(seq_len):\n",
    "\n",
    "                 # read the video\n",
    "                ret, frame = capture.read()\n",
    "\n",
    "                # make detections with the holistic model\n",
    "                image, results = mp_detection(frame, holistic)\n",
    "\n",
    "                # draw landmarks on the live feed\n",
    "                draw_landmarks(image, results)\n",
    "\n",
    "                # output text to frame\n",
    "                cv2.putText(image, '{}: {}'.format(action, sequence), (15, 12),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "                # show the video in frame\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "                # extract keypoints for each collection sample\n",
    "                keypoints = get_single_hand_keypoints(results)\n",
    "                npy_path = os.path.join(PATH, action, str(sequence), str(frame_nr))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                # escape the video feed\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c03907",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1724cb87",
   "metadata": {},
   "source": [
    "Now we have all the data we need saved in a folder on the computer. We need to preprocess them in order to feed them to the recognition model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac115361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'right': 0, 'left': 1, 'up': 2, 'down': 3}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map the actions to numbers for labels\n",
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10d06e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra ???\n",
    "\n",
    "# sequences = x, labels =y\n",
    "sequences, labels = [], []\n",
    "\n",
    "# loop through all the data we collected and save them to the sequences array,\n",
    "# \n",
    "for action in actions:\n",
    "    for sequence in range(seq_nr*2):\n",
    "        \n",
    "        # get together all arrays for that particular video\n",
    "        window = []\n",
    "        \n",
    "        # load and append all frames for a single video\n",
    "        for frame_nr in range(seq_len):\n",
    "            res = np.load(os.path.join(PATH, action, str(sequence), '{}.npy'.format(frame_nr)))\n",
    "            window.append(res)\n",
    "            \n",
    "        # append each video to the sequences (x data)\n",
    "        sequences.append(window)\n",
    "        \n",
    "        # append label for the given video to the labels (y data)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a0b7b994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences = x, labels =y\n",
    "sequences, labels = [], []\n",
    "\n",
    "# loop through all the data we collected and save them to the sequences array,\n",
    "# and save the corresponding label to the labels array\n",
    "for action in actions:\n",
    "    for sequence in range(seq_nr*2):\n",
    "        res = np.load(os.path.join(PATH, action, str(sequence), '{}.npy'.format(frame_nr)))\n",
    "        sequences.append(res)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a1b85e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 21, 3)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it should have the shape of (x, y, z), where\n",
    "# x is the number of collected videos (single frames), \n",
    "# y is the number of keypoints in each sample\n",
    "# and z is the number of dimensions per each keypoint\n",
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "be920913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000,)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the labels should have a single dimension matching x from the shape of sequences\n",
    "np.array(labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a7a488ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get numpy array of video data and reshape it for input into the model\n",
    "X = np.array(sequences).reshape(-1, 21, 3, 1)\n",
    "\n",
    "# get one-hot encoding of label variables\n",
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b1f6c3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train and test partitions\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f783cb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape matching: True\n"
     ]
    }
   ],
   "source": [
    "# double check train and test shapes to make sure they match up\n",
    "print('Data shape matching: {}'.format(X_train.shape[0] == y_train.shape[0] and X_test.shape[0] == y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff1a40f",
   "metadata": {},
   "source": [
    "# Model and training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caf5c0d",
   "metadata": {},
   "source": [
    "Now that we have the data processed, it is time to define a machine learning model for multilabel classification. I experimented with several different architectures, including an LSTM model before I decided to use single frames individually (originally, I wanted to record each gesture as a sequence of multiple frames). I ended up deciding on a deep learning convolutional network which trained fairly quickly and offered good performance without overfitting on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9a09fd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging and callback\n",
    "log_dir = os.path.join('logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9dc03659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_model(input_shape, output_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Input(input_shape))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(output_shape, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d53c658",
   "metadata": {},
   "source": [
    "The resulting model has 4 different components:\n",
    "\n",
    "1. Convolutional layers\n",
    "Convolutional layers apply convolutions to the input, passing a learnable filter over it. It is used to learn patterns from the input. It is common for use in image and spatial data. Since I am not flattening the array of keypoints but keeping it in its x, y and z components, I thought it would be a good choice.\n",
    "\n",
    "2. Dropout layers\n",
    "Dropout layers randomly set some layer activations to 0, helping the model learn multiple independent representations and reduce dependency on any single parameter. I added them to help avoid overfitting.\n",
    "\n",
    "3. Pooling layer\n",
    "I chose GlobalAveragePooling2D as my pooling layer. Since the data is fairly small, a simple pooling layer like this seemed appropriate. It also further acts as a regularisation layer, decreasing the dependency on one single parameter. Lastly, as the model will be used for live predictions, I chose it for its computational efficiency.\n",
    "\n",
    "4. Dense (fully connected) layers\n",
    "Dense layers were introduced to perform classification on the features extracted by the previous layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b0be9ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 21, 3, 1)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "550ad4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_model((X.shape[1], X.shape[2], X.shape[3]), y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "855f03a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_13 (Conv2D)          (None, 19, 1, 32)         320       \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 19, 1, 128)        4224      \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 19, 1, 128)        0         \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 19, 1, 64)         8256      \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 19, 1, 64)         0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 19, 1, 32)         2080      \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 19, 1, 16)         528       \n",
      "                                                                 \n",
      " global_average_pooling2d_2   (None, 16)               0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 4)                 68        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,476\n",
      "Trainable params: 15,476\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fc8e4a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical crossentropy loss necessary for a categorical classification model\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "454348da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "119/119 [==============================] - 40s 333ms/step - loss: 0.1064 - categorical_accuracy: 0.9647\n",
      "Epoch 2/30\n",
      "119/119 [==============================] - 38s 323ms/step - loss: 0.0939 - categorical_accuracy: 0.9734\n",
      "Epoch 3/30\n",
      "119/119 [==============================] - 36s 302ms/step - loss: 0.0800 - categorical_accuracy: 0.9763\n",
      "Epoch 4/30\n",
      "119/119 [==============================] - 39s 326ms/step - loss: 0.0937 - categorical_accuracy: 0.9692\n",
      "Epoch 5/30\n",
      "119/119 [==============================] - 38s 323ms/step - loss: 0.1031 - categorical_accuracy: 0.9679\n",
      "Epoch 6/30\n",
      "119/119 [==============================] - 39s 325ms/step - loss: 0.0834 - categorical_accuracy: 0.9732\n",
      "Epoch 7/30\n",
      "119/119 [==============================] - 39s 329ms/step - loss: 0.0945 - categorical_accuracy: 0.9718\n",
      "Epoch 8/30\n",
      "119/119 [==============================] - 42s 355ms/step - loss: 0.0868 - categorical_accuracy: 0.9726\n",
      "Epoch 9/30\n",
      "119/119 [==============================] - 40s 338ms/step - loss: 0.0970 - categorical_accuracy: 0.9692\n",
      "Epoch 10/30\n",
      "119/119 [==============================] - 40s 339ms/step - loss: 0.0997 - categorical_accuracy: 0.9695\n",
      "Epoch 11/30\n",
      "119/119 [==============================] - 40s 340ms/step - loss: 0.0907 - categorical_accuracy: 0.9703\n",
      "Epoch 12/30\n",
      "119/119 [==============================] - 40s 333ms/step - loss: 0.0629 - categorical_accuracy: 0.9837\n",
      "Epoch 13/30\n",
      "119/119 [==============================] - 38s 320ms/step - loss: 0.0678 - categorical_accuracy: 0.9795\n",
      "Epoch 14/30\n",
      "119/119 [==============================] - 39s 327ms/step - loss: 0.0714 - categorical_accuracy: 0.9768\n",
      "Epoch 15/30\n",
      "119/119 [==============================] - 38s 317ms/step - loss: 0.0575 - categorical_accuracy: 0.9837\n",
      "Epoch 16/30\n",
      "119/119 [==============================] - 37s 313ms/step - loss: 0.0555 - categorical_accuracy: 0.9842\n",
      "Epoch 17/30\n",
      "119/119 [==============================] - 37s 311ms/step - loss: 0.0649 - categorical_accuracy: 0.9824\n",
      "Epoch 18/30\n",
      "119/119 [==============================] - 37s 307ms/step - loss: 0.0696 - categorical_accuracy: 0.9789\n",
      "Epoch 19/30\n",
      "119/119 [==============================] - 36s 303ms/step - loss: 0.0577 - categorical_accuracy: 0.9821\n",
      "Epoch 20/30\n",
      "119/119 [==============================] - 36s 305ms/step - loss: 0.0509 - categorical_accuracy: 0.9858\n",
      "Epoch 21/30\n",
      "119/119 [==============================] - 36s 302ms/step - loss: 0.0552 - categorical_accuracy: 0.9826\n",
      "Epoch 22/30\n",
      "119/119 [==============================] - 37s 312ms/step - loss: 0.0571 - categorical_accuracy: 0.9826\n",
      "Epoch 23/30\n",
      "119/119 [==============================] - 38s 322ms/step - loss: 0.0513 - categorical_accuracy: 0.9850\n",
      "Epoch 24/30\n",
      "119/119 [==============================] - 39s 329ms/step - loss: 0.0384 - categorical_accuracy: 0.9892\n",
      "Epoch 25/30\n",
      "119/119 [==============================] - 37s 314ms/step - loss: 0.0446 - categorical_accuracy: 0.9874\n",
      "Epoch 26/30\n",
      "119/119 [==============================] - 36s 302ms/step - loss: 0.0516 - categorical_accuracy: 0.9850\n",
      "Epoch 27/30\n",
      "119/119 [==============================] - 39s 325ms/step - loss: 0.0328 - categorical_accuracy: 0.9924\n",
      "Epoch 28/30\n",
      "119/119 [==============================] - 37s 311ms/step - loss: 0.0796 - categorical_accuracy: 0.9734\n",
      "Epoch 29/30\n",
      "119/119 [==============================] - 37s 312ms/step - loss: 0.0373 - categorical_accuracy: 0.9905\n",
      "Epoch 30/30\n",
      "119/119 [==============================] - 35s 293ms/step - loss: 0.0403 - categorical_accuracy: 0.9884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2428aa7f970>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2aae7f",
   "metadata": {},
   "source": [
    "Training for 100 epochs provides a consistently good result with high accuracy and low loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "7505f8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "517dcbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'right'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(res[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d4e51575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'right'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(y_test[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5c4af7",
   "metadata": {},
   "source": [
    "# Save/load weigths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41a9ffd",
   "metadata": {},
   "source": [
    "To use the model later, I save the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "102de59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('actions_model-single_hand.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "fd713567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights('actions_model-single_hand.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca675391",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b138794",
   "metadata": {},
   "source": [
    "It is also important to evaluate the accuracy of the model on the test data. We can confirm that the model we trained provides high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "49984c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0a19cd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from one-hot encoding back into categorical\n",
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3ffedecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[149,   0],\n",
       "        [  0,  51]],\n",
       "\n",
       "       [[156,   0],\n",
       "        [  0,  44]],\n",
       "\n",
       "       [[145,   1],\n",
       "        [  0,  54]],\n",
       "\n",
       "       [[149,   0],\n",
       "        [  1,  50]]], dtype=int64)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix to see true and false positives/negatives\n",
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "643fe5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.995"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get accuracy\n",
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac231f02",
   "metadata": {},
   "source": [
    "# Real-time detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c294b896",
   "metadata": {},
   "source": [
    "The last thing to do is set up real time detection and prediction. We will utilise a camera live feed loop similar to the one we used to collect the data in the beginning. We will also visualise the probabilities of the individual predictions for easier use and transparency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c71d92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(0,127,255), (0,255,0), (255,255,0), (255,0,127), (127,0,255)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    '''\n",
    "    Visualise the probabilities of the individual categories for the predictions for a given frame.\n",
    "        \n",
    "        Parameters:\n",
    "            res (numpy.ndarray): an array with the probability predictions for each category\n",
    "            actions (array): an array containing the name of the individual categories. Has to be of the same \n",
    "                length as the colors array\n",
    "            input_frame (numpy.ndarray): an ndarray representing an image for which we want to make a prediction\n",
    "            colors (array): an array of tuple representations of colors to visualise individual categories. Has be of the same\n",
    "                length as the actions array\n",
    "                \n",
    "        Returns:\n",
    "            otuput_frame (numpy.ndarray): an ndarray with overlay of visualised probabilities for each action\n",
    "    '''\n",
    "    \n",
    "    output_frame = input_frame.copy()\n",
    "    \n",
    "    for i in range(len(res[0])):\n",
    "        cv2.rectangle(output_frame, (0,60+i*40), (int(res[0][i]*100), 90+i*40), colors[i], -1)\n",
    "        cv2.putText(output_frame, actions[i], (0, 85+i*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "1594f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access webcam\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "predictions = []\n",
    "frameNr = 0\n",
    "history = []\n",
    "threshold = 0.7\n",
    "\n",
    "# access mediapipe holistic model\n",
    "# set initial detection confidence and subsequent tracking confidence\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while capture.isOpened():\n",
    "\n",
    "        # read the video\n",
    "        ret, frame = capture.read()\n",
    "\n",
    "        # make detections with the holistic model\n",
    "        image, results = mp_detection(frame, holistic)\n",
    "        \n",
    "        # draw landmarks on the live feed\n",
    "        draw_landmarks(image, results)\n",
    "        \n",
    "        keypoints = get_single_hand_keypoints(results)\n",
    "        \n",
    "        res = model.predict(keypoints.reshape(-1, 21, 3, 1), verbose=0)\n",
    "        predictions.append(np.argmax(res))\n",
    "        \n",
    "        if res[0][np.argmax(res)] > threshold:\n",
    "            if len(history) > 0:\n",
    "                if actions[np.argmax(res)] != history[-1]:\n",
    "                    history.append(actions[np.argmax(res)])\n",
    "            else:\n",
    "                history.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(history) > 5:\n",
    "                history = history[-5:]\n",
    "                \n",
    "        # visualise probabilities\n",
    "        image = prob_viz(res, actions, image, colors)\n",
    "        \n",
    "        # show the video in frame\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # escape the video feed\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
